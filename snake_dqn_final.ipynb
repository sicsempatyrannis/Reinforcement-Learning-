{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8QIjfYjRD7C"
   },
   
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "from IPython.display import clear_output\n",
    "import time \n",
    "from matplotlib import colors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "from collections import deque\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 975,
     "status": "ok",
     "timestamp": 1588605537566,
     "user": {
      "displayName": "Raymond Ezra",
      "photoUrl": "",
      "userId": "10477477276504387985"
     },
     "user_tz": -60
    },
    "id": "QxiQbrc0RD7H",
    "outputId": "08d08693-82bd-4c91-80d6-d9518dfc5fcc"
   },
   "outputs": [],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1taPO5BRD7K"
   },
   "outputs": [],
   "source": [
    "def render_env(grid, snake, food, sleep_time : float = 0.1) :\n",
    "    \"\"\"\n",
    "    Render taken from one of the labs\n",
    "\n",
    "    \"\"\"\n",
    "    # Turn interactive mode on.\n",
    "    plt.ion()\n",
    "    fig = plt.figure(num = \"env_render\")\n",
    "    ax = plt.gca()\n",
    "    ax.clear()\n",
    "    clear_output(wait = True)\n",
    "\n",
    "    \n",
    "    env_plot = np.copy(grid)\n",
    "    for i in snake:\n",
    "        env_plot[tuple(i)] = 2\n",
    "    env_plot[tuple(food)] = 3\n",
    "\n",
    "    # Plot the gridworld.\n",
    "    cmap = colors.ListedColormap([\"grey\", \"black\", \"white\", \"green\"])\n",
    "    bounds = list(range(5))\n",
    "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "    ax.imshow(env_plot, cmap = cmap, norm = norm, zorder = 0)\n",
    "\n",
    "    \n",
    "    # Set up axes.\n",
    "    ax.grid(which = 'major', axis = 'both', linestyle = '-', color = 'k', linewidth = 2, zorder = 1)\n",
    "    ax.set_xticks(np.arange(-0.5, grid.shape[1] , 1));\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticks(np.arange(-0.5, grid.shape[0], 1));\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Sleep if desired.\n",
    "    if (sleep_time > 0) :\n",
    "        time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXjTpuNVRD7N"
   },
   "outputs": [],
   "source": [
    "class SnakeEnv():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # game board, for rendering\n",
    "        self.grid_height = 20 \n",
    "        self.grid_width = 20 \n",
    "        self.grid = np.zeros((self.grid_height, self.grid_width), dtype = int)\n",
    "        \n",
    "        self.grid[1:-1,1:-1] = 1\n",
    "        \n",
    "        # add random extra walls\n",
    "        # self.grid[4,6] = 0\n",
    "        \n",
    "        self.walls = np.argwhere(self.grid == 0).tolist()\n",
    "        # place snake randomly\n",
    "        self.snake = [[random.randint(1, self.grid_height-2), random.randint(1, self.grid_width-2)]]\n",
    "        \n",
    "        self.snake_orientation = np.random.randint(0,4,1)[0]\n",
    "        \n",
    "        # place food somewhere the snake isnt\n",
    "        food = None\n",
    "        while food is None:\n",
    "            nf = [random.randint(1, self.grid_height-2),random.randint(1, self.grid_width-2)]\n",
    "            food = nf if nf not in self.snake else None\n",
    "        self.food = food\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        # place snake randomly\n",
    "        self.snake = [[random.randint(1, self.grid_height-2), random.randint(1, self.grid_width-2)]]\n",
    "        \n",
    "        # place food somewhere the snake isnt\n",
    "        food = None\n",
    "        while food is None:\n",
    "            nf = [random.randint(1, self.grid_height-2),random.randint(1, self.grid_width-2)]\n",
    "            food = nf if nf not in self.snake else None\n",
    "        self.food = food\n",
    "        \n",
    "        return self.get_state()\n",
    "    \n",
    "    def sample(self):\n",
    "        return np.random.randint(0,3,1)[0]\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        actions = ['left','foward','right']\n",
    "        action = actions[action]\n",
    "        new_head = [self.snake[0][0], self.snake[0][1]]\n",
    "        \n",
    "        \n",
    "        def step_ori(orientation, new_head):\n",
    "            if orientation == 2:\n",
    "                new_head[0] += 1\n",
    "            if orientation == 0:\n",
    "                new_head[0] -= 1\n",
    "            if orientation == 3:\n",
    "                new_head[1] -= 1\n",
    "            if orientation == 1:\n",
    "                new_head[1] += 1\n",
    "                \n",
    "            return new_head\n",
    "        \n",
    "        if action == 'left':\n",
    "            self.snake_orientation = (self.snake_orientation - 1) % 4\n",
    "        \n",
    "        if action == 'right':\n",
    "            self.snake_orientation = (self.snake_orientation + 1) % 4\n",
    "        \n",
    "        new_head = step_ori(self.snake_orientation, new_head)\n",
    "\n",
    "        self.snake.insert(0, new_head)\n",
    "        \n",
    "        if self.snake[0] in self.walls or self.snake[0] in self.snake[1:]:\n",
    "            # reward, terminal\n",
    "            return (-1., True) \n",
    "        \n",
    "        elif self.snake[0] == self.food:\n",
    "            food = None\n",
    "            while food is None:\n",
    "                nf = [\n",
    "                    random.randint(1, self.grid_height-2),\n",
    "                    random.randint(1, self.grid_width-2)\n",
    "                ]\n",
    "                food = nf if nf not in self.snake else None\n",
    "            self.food = food\n",
    "            return (100., False) \n",
    "        else:\n",
    "            tail = self.snake.pop()\n",
    "            return (-0.01, False) \n",
    "    \n",
    "        \n",
    "    def get_state(self):\n",
    "        \n",
    "        \n",
    "        head = np.array([self.snake[0][0], self.snake[0][1]])\n",
    "        tail = np.array([self.snake[-1][0], self.snake[-1][1]])\n",
    "        \n",
    "        food = np.array(self.food[:])\n",
    "        walls = np.array(self.walls[:])\n",
    "        state = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        temp = [np.array([-1,0]), np.array([-1,1]), np.array([0,1]), np.array([1,1]), \\\n",
    "                np.array([1,0]), np.array([1,-1]), np.array([0,-1]), np.array([-1,-1]) ]\n",
    "        \n",
    "        temp = temp+temp\n",
    "        \n",
    "        directions = temp[2*self.snake_orientation : 2*self.snake_orientation + 8]\n",
    "        \n",
    "        \n",
    "        \n",
    "        for direction in directions:\n",
    "            pos = head + direction\n",
    "            # walls\n",
    "            shortest_wall = np.inf\n",
    "            for wall in self.walls:\n",
    "                if np.linalg.norm(pos-wall) < shortest_wall:\n",
    "                    shortest_wall = np.linalg.norm(pos-wall)\n",
    "            state.append(shortest_wall)\n",
    "\n",
    "            \n",
    "            # food\n",
    "            \n",
    "            state.append(np.linalg.norm(pos-food))\n",
    "            \n",
    "            # tail\n",
    "            \n",
    "            state.append(np.linalg.norm(pos-tail))\n",
    "            \n",
    "        \n",
    "        return np.array(state)\n",
    "                    \n",
    "        \n",
    "    \n",
    "    def render(self):\n",
    "        render_env(self.grid, self.snake, self.food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCuq3UfqRD7U"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, max_length):\n",
    "        self.memory = deque(maxlen = max_length)\n",
    "\n",
    "    def append_mem(self, transition):\n",
    "        # transition (state,next_state,action,reward,terminal)\n",
    "        self.memory.append(transition)\n",
    "        \n",
    "    def sample_minibatch(self, minibatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for i in range(minibatch_length):\n",
    "            random_int = np.random.randint(0, len(self.memory)-1) \n",
    "            transition = self.memory[random_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            rewards.append(transition[3])\n",
    "            terminals.append(transition[4])\n",
    "        return torch.Tensor(states).cuda(), torch.Tensor(next_states).cuda(), torch.Tensor(actions).cuda(), torch.Tensor(rewards), torch.Tensor(terminals).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vg22np5vRD7X"
   },
   "outputs": [],
   "source": [
    "class Net11(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net11, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(in_features = 24, out_features = 18, bias = True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(in_features = 18, out_features = 3, bias = True),\n",
    "                        nn.Softmax(dim=1)\n",
    "                        )\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9eQVXz8ORD7a"
   },
   "outputs": [],
   "source": [
    "class Net13(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net13, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(in_features = 24, out_features = 18, bias = True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(in_features = 18, out_features = 18, bias = True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(in_features = 18, out_features =3, bias = True),\n",
    "                        nn.Softmax(dim=1)\n",
    "                        )\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HClBx2qKRD7d"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njEoh45tRD7f"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "\n",
    "    def __init__(self, network = Net13().apply(init_weights).cuda()):\n",
    "\n",
    "        self.name = 'net13_{}'.format(time.time())\n",
    "        # hyperparameters\n",
    "        \n",
    "        # network hyperparameters\n",
    "        self.n_learning_rate = 0.001\n",
    "\n",
    "        # q-learning hyperparameters\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # memory\n",
    "        self.memory_capacity = 50000\n",
    "        self.replay_memory = ReplayMemory(self.memory_capacity)\n",
    "\n",
    "        # networks\n",
    "        # optional update target net every N episodes\n",
    "        self.target_network_update = 20\n",
    "        # initialise action-value net\n",
    "        self.network = network\n",
    "        \n",
    "        # initialise target action-value net with weights of original network\n",
    "        self.target_network = copy.deepcopy(self.network).cuda()\n",
    "\n",
    "        self.network_optimiser = torch.optim.Adam( self.network.parameters(), lr=self.n_learning_rate)\n",
    "        self.MSELoss_function = nn.MSELoss().cuda()\n",
    "        \n",
    "        self.tau = 0.99 #0.99\n",
    "        self.epsilon = 0.1 #0.1\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            # choose random action\n",
    "            return random.choice([0,1,2])\n",
    "        else:\n",
    "            \n",
    "            # network_output = self.network(state).clone().detach().cpu().data.numpy()\n",
    "            network_output = self.network(state).cpu().data.numpy()\n",
    "            return np.argmax(network_output)\n",
    "    \n",
    "    def update_QN(self, state, next_state, action, reward, terminals):\n",
    "        \n",
    "        qsa = torch.gather(self.network(state).cuda(), dim=1, index=action.long().cuda())\n",
    "        qsa_next_action = self.target_network(next_state)\n",
    "        qsa_next_action,_ = torch.max(qsa_next_action, dim=1, keepdim=True)\n",
    "        \n",
    "        not_terminals = 1 - terminals\n",
    "        \n",
    "        qsa_next_target = reward.cpu() + not_terminals.cpu() * self.gamma * qsa_next_action.cpu()\n",
    "        \n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach().cuda())\n",
    "        \n",
    "        self.network_optimiser.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        self.network_optimiser.step()\n",
    "    \n",
    "    def soft_target_update(self, network, target_network, tau):\n",
    "        for net_params, target_net_params in zip(network.parameters(), target_network.parameters()):\n",
    "            target_net_params.data.copy_(net_params.data * tau + target_net_params.data * (1 - tau))\n",
    "            \n",
    "    def update(self, update_rate):\n",
    "        for i in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.replay_memory.sample_minibatch(128)\n",
    "            self.update_QN(states, next_states, actions, rewards, terminals)\n",
    "            self.soft_target_update(self.network, self.target_network, self.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QYOmvjzbRD7h"
   },
   "outputs": [],
   "source": [
    "def train_agent(number_of_episodes, agent = DQN(), max_time_steps = 500, env= SnakeEnv()):\n",
    "    food_sum_list = []\n",
    "    for episode in range(number_of_episodes):\n",
    "        reward_sum = 0\n",
    "        food_sum = 0\n",
    "        time_step = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        while time_step < max_time_steps:\n",
    "            \n",
    "            nn_input = torch.tensor(state).type('torch.FloatTensor').view(1,-1).cuda()\n",
    "            \n",
    "            # for pretrained change epsilon to 0.05\n",
    "            action = agent.epsilon_greedy_action( nn_input, 0.05) \n",
    "            \n",
    "            reward, terminal = env.step(action)\n",
    "            next_state = env.get_state()\n",
    "\n",
    "            reward_sum += reward\n",
    "            agent.replay_memory.append_mem( (state,next_state,[action],[reward],[terminal]) )\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            time_step += 1\n",
    "            if reward > 5:\n",
    "                food_sum += 1\n",
    "                time_step = 0\n",
    "\n",
    "            if terminal:\n",
    "                clear_output(wait=True)\n",
    "                print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum, 'food collected in episode:', food_sum)\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        print('Updating Target Network')\n",
    "        agent.update(40)\n",
    "        food_sum_list.append(food_sum)\n",
    "        \n",
    "    return agent, food_sum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_agent(agent, show = True):\n",
    "    env = SnakeEnv()\n",
    "    if show:\n",
    "        env.render()\n",
    "    \n",
    "    \n",
    "    food_sum_list = []\n",
    "    for i in range(100):\n",
    "        c_reward = 0\n",
    "        food_sum = 0\n",
    "        time_step = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        while time_step < 500:\n",
    "            state = env.get_state()\n",
    "            nn_input = torch.tensor(state).type('torch.FloatTensor').view(1,-1).cuda()\n",
    "            \n",
    "            action = agent.epsilon_greedy_action( nn_input, 0)\n",
    "            reward, terminal = env.step(action)\n",
    "            \n",
    "            if reward > 5:\n",
    "                food_sum += 1\n",
    "                time_step = 0\n",
    "            \n",
    "            if show:\n",
    "                env.render()\n",
    "\n",
    "            c_reward += reward\n",
    "            if terminal:\n",
    "                break\n",
    "        \n",
    "        food_sum_list.append(food_sum)\n",
    "        \n",
    "    return food_sum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agent(agent, network_name):\n",
    "    torch.save(agent.network.state_dict(),  network_name+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_snake0_05, food_list = train_agent(10000)\n",
    "save_agent(new_snake0_05, 'best_10000')\n",
    "with open(\"best.txt\", \"w\") as output:\n",
    "    output.write(str(food_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((np.cumsum(food_list)/np.arange(1,len(food_list)+1)))\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Cumulative Mean Food Collected')\n",
    "plt.savefig('goodone_epsilon05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_food_sum = show_agent(new_snake0_05, False)\n",
    "\n",
    "print('Mean', np.mean(total_food_sum))\n",
    "print('Min', min(total_food_sum))\n",
    "print('Max', max(total_food_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-AaRexjQRD7w"
   },
   "outputs": [],
   "source": [
    "# random agent\n",
    "env = SnakeEnv()\n",
    "#env.render()\n",
    "food_list = []\n",
    "for i in range(100):\n",
    "    c_reward = 0\n",
    "    food_sum = 0\n",
    "    time_step = 0\n",
    "    env.reset()\n",
    "    \n",
    "    while time_step < 500:\n",
    "        \n",
    "        reward, terminal = env.step(random.choice([0,1,2]))\n",
    "        #env.render()\n",
    "        \n",
    "        c_reward += reward\n",
    "        if terminal:\n",
    "            break\n",
    "            \n",
    "        if reward > 5:\n",
    "            food_sum += 1\n",
    "            time_step = 0\n",
    "    \n",
    "    food_list.append(food_sum)\n",
    "    \n",
    "print('Mean', np.mean(food_list))\n",
    "print('Min', min(food_list))\n",
    "print('Max', max(food_list))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "snake_dqn_colab2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
